\section{\texttt{word2vec}}
\label{sec:Chapter2} \index{Chapter2}

Модель \texttt{word2vec} основана на использовании двухслойной нейронной сети прямого распространения. В соответствующем инструменте реализованы архитектуры \textit{Continuous Bag-of-Words} и \textit{Skip-gram} \cite{mikolov1} для получения векторного представления слов фиксированной размерности \(d\). Обучение при помощи \texttt{word2vec} является онлайновым: имеется скользящее по обучающему корпусу текстов симметричное контекстное окно переменного размера.

Будем рассматривать архитектуру \textit{Skip-gram}, поскольку она является более качественной \cite{mikolov1}. Предсказание текущего слова \(\omega_i\) в этой архитектуре разбивается на отдельные предсказания этого слова на основе каждого из слов контекста: \[\log P(\omega_i|\omega_1^{i-1}) = \log P(\omega_i|context(\omega_i)) = \sum_{j=-k}^k \log P(\omega_i|\omega_{i+j}).\]

Для ускорения обучения при вычислении \(P(\omega_i|\omega_{i+j})\) вместо применения \textit{softmax}-преобразования над всем словарём (как это делается, например, в ранней модели \cite{bengio}) предлагаются подходы \textit{hierarchical softmax} и \textit{negative sampling}, второй из которых является более качественным \cite{mikolov2}.

Задачей \textit{negative sampling} является отделение положительного примера \((\omega_i, \omega_{i+j})\) от \(s\) отрицательных примеров \((\omega_{neg_1}, \omega_{i+j}), ..., (\omega_{neg_s}, \omega_{i+j})\) на основе логистической регрессии, слова \(\omega_{neg_l}\) выбираются случайным образом из некоторого распределения над словарём \cite{negative}: \[\log P(\omega_i|\omega_{i+j}) = \log \sigma (v(\omega_i) v(\omega_{i+j})) + \sum_{l=1}^s \log \sigma (- v(\omega_{neg_l}) v(\omega_{i+j})).\]

Здесь \(v(\omega) \in \mathbb{R}^d\) -- векторное представление слова \(\omega\), \(\sigma(x) = \frac{1}{1+e^{-x}}\) -- логистическая функция, а под произведением векторов понимается их скалярное произведение.

Особенностью векторных представлений \texttt{word2vec} является возможность решать на их основе задачи поиска синонимов и аналогий слов. Например, к вектору \(v(king)-v(man)+v(woman)\) наиболее близким по косинусной мере среди векторов слов оказывается \(v(queen)\) \cite{mikolov3}.