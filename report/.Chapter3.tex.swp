\section{\texttt{word2vecf}}
\label{sec:Chapter3} \index{Chapter3}

Использование линейного контекста является существенным недостатком \texttt{word2vec}. Модель \texttt{word2vecf} \cite{levy1} вместо этого использует контекст на основе деревьев зависимостей предложений. При таком подходе появляется возможность в большей мере учитывать зависимости между словами в предложении и в меньшей мере учитывать близость в предложении двух слов, слабо связанных синтаксически.

Будем работать с деревьями зависимостей в формате конференции CoNLL-X \cite{format}. Для каждой пары слов (\texttt{first}, \texttt{second}), в которой второе слово зависит от первого с типом \texttt{type}, составим две пары:

\begin{center}
\(p_1 = \) (\texttt{first}, \texttt{type\char`_second});

\(p_2 = \) (\texttt{second}, \texttt{typeI\char`_first}).
\end{center}

Составим на основе корпуса зависимостей в формате CoNLL-X обучающий корпус пар, состоящий из пар вида \(p_1\) и \(p_2\) для всех имеющихся зависимостей. Составленный корпус и будет являться входными данными для инструмента \texttt{word2vecf}.

Инструмент использует два словаря -- словарь слов (множество всех различных первых элементов пар в корпусе пар) и словарь контекстов (множество всех различных вторых элементов). Каждому из элементов в обоих словарях сопоставляется вещественнозначный вектор размерности \(d\). Векторные представления слов и контекстов составляют веса первого и второго слоя нейронной сети соответственно.

В процессе обучения нейронная сеть предсказывает для текущей пары первый её элемент на основе второго. При этом из \texttt{word2vec} был заимствован подход \textit{negative sampling} для ускорения обучения:
\[\log P(\omega_i|c_i) = \log \sigma (v(\omega_i) v(c_i)) + \sum_{l=1}^s \log \sigma (- v(\omega_{neg_l}) v(c_i)).\]

Обучающий корпус пар может быть построен на основе корпуса зависимостей при помощи скрипта на языке Python 2, входящего в инструмент \texttt{word2vecf}.