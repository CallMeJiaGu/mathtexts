\documentclass[oneside,senior,etd]{BYUPhys}

\usepackage[utf8]{inputenc}
\usepackage{rotating} 

\usepackage[russian]{babel}
\usepackage{amsfonts} % Пакеты для математических символов и теорем
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx} % Пакеты для вставки графики
\usepackage{subfig}
\usepackage{color}
\usepackage[unicode]{hyperref}
\usepackage[nottoc]{tocbibind} % Для того, чтобы список литературы отображался в оглавлении
\usepackage{algorithmic} % Для записи алгоритмов в псевдокоде
\usepackage{algorithm}
\usepackage{verbatim} % Для вставок заранее подготовленного текста в режиме as-is

\Chair{Кафедра Системного Программирования}
\Lab{~}
\Year{2015}
  \Month{Месяц}
  \City{Москва}
  \AuthorText{Автор}
  \Author{Архипенко Константин Владимирович}
  \AuthorEng{Konstantin Arkhipenko}
  \AcadGroup{427}

  \TitleTop{Модель векторного представления слов на основе}
  %\TitleMiddle{определения синонимов в комментариях}
  \TitleBottom{деревьев зависимостей предложений} % leave empty if you don't need it
  \TitleTopEng{Thesis theme, first line}
  %\TitleMiddleEng{Thesis theme, second line}
  \TitleBottomEng{Thesis theme, third line} % leave empty if you don't need it
  
 \docname{Отчёт по заданию}
  \Advisor{Масленников Мстислав Владимирович}
  \AdvisorDegree{Dr\\}

%\Abstract{Текст аннотации на русском}
%\AbstractEng{Abstract in english}


%%%% DON'T change this. It is here because .sty does not support cyrillic cp properly %%%%
\University{Московский Государственный Университет имени М.В.Ломоносова}
\Faculty{Факультет Вычислительной Математики и Кибернетики}
\GrText{студент гр.}
\AdvisorText{Преподаватель}
%\AbstractText{Аннотация}

\begin{document}
\fixmargins
 \makepreliminarypages

\oneandhalfspace

\tableofcontents


\section{Введение}
\label{sec:Chapter1} \index{Chapter1}

А этой работе мы рассматриваем проблему поиска контекстуальных синонимов для заданного слова в контексте. Например, в предложении ``миру мир'' слову ``миру'' должны быть подобраны синонимы ``планете'', а слову ``мир'' - синоним ``согласие''. Одним из распространённых подходов является \texttt{word2vec}\footnote{\url{https://code.google.com/p/word2vec/}}  от \cite{mikolov1}.

Модель \texttt{word2vec} основана на использовании двухслойной нейронной сети прямого распространения. В соответствующем инструменте реализованы архитектуры \textit{Continuous Bag-of-Words} и \textit{Skip-gram} \cite{mikolov1} для получения векторного представления слов фиксированной размерности \(d\). Обучение при помощи \texttt{word2vec} является онлайновым: имеется скользящее по обучающему корпусу текстов симметричное контекстное окно переменного размера.

Будем рассматривать архитектуру \textit{Skip-gram}, поскольку она является более качественной \cite{mikolov1}. Предсказание текущего слова \(\omega_i\) в этой архитектуре разбивается на отдельные предсказания этого слова на основе каждого из слов контекста: \[\log P(\omega_i|\omega_1^{i-1}) = \log P(\omega_i|context(\omega_i)) = \sum_{j=-k}^k \log P(\omega_i|\omega_{i+j}).\]

Для ускорения обучения при вычислении \(P(\omega_i|\omega_{i+j})\) вместо применения \textit{softmax}-преобразования над всем словарём (как это делается, например, в ранней модели \cite{bengio}) предлагаются подходы \textit{hierarchical softmax} и \textit{negative sampling}, второй из которых является более качественным \cite{mikolov2}.

ПУсть $\omega_i$ и $c_i$ обозначают слово и его контекст, соответственно. Задачей \textit{negative sampling} является отделение положительного примера \((\omega_i, \omega_{i+j})\) от \(s\) отрицательных примеров \((\omega_{neg_1}, \omega_{i+j}), ..., (\omega_{neg_s}, \omega_{i+j})\) на основе логистической регрессии, слова \(\omega_{neg_l}\) выбираются случайным образом из некоторого распределения над словарём \cite{negative}: \[\log P(\omega_i|\omega_{i+j}) = \log \sigma (v(\omega_i) v(\omega_{i+j})) + \sum_{l=1}^s \log \sigma (- v(\omega_{neg_l}) v(\omega_{i+j})).\]
Здесь \(v(\omega) \in \mathbb{R}^d\) -- векторное представление слова \(\omega\), \(\sigma(x) = \frac{1}{1+e^{-x}}\) -- логистическая функция, а под произведением векторов понимается их скалярное произведение.

В результате, благодаря векторному представлению \texttt{word2vec} можно находить синонимы и аналогии заданного слова. Например, к вектору \(v(king)-v(man)+v(woman)\) наиболее близким по косинусной мере среди векторов слов оказывается \(v(queen)\) \cite{mikolov3}.




При использовании \texttt{word2vec} возникает проблема: При построении векторного представления слова с помощью двухслойной нейронной сети \texttt{word2vec}  от \cite{mikolov1} многие подобранные слова-синонимы не взаимозаменяемы из-за различной синтаксической формы. Например, для слова ``ключ'' \texttt{word2vec} выдаёт синонимы ``ключа'', ``ключи'' и ``ключом'' на 2-4 позициях, соответственно. 
% пароль, пистолет, кристалл, шум

% сообщил -> сообщила, сообщили, сообщает, сообщив
%            рассказал, заявил, сообщал, доложил, отметил

Чтобы её решить, Levi et al.~\shortcite{levy1} в программе \texttt{word2vecf}\footnote{\url{https://bitbucket.org/yoavgo/word2vecf/}}  используют контекст на основе деревьев зависимостей предложений. При таком подходе появляется возможность в большей мере учитывать зависимости между словами в предложении и в меньшей мере учитывать близость в предложении двух слов, слабо связанных синтаксически.
Согласно Levi et al.~\shortcite{levy1}, она лучше справляется с задачей поиска синонимов слов. Также, Melamud et al.~\shortcite{melamud} использовали информацию о синтаксических связях окружающих слов в предложении. 
Однако, эта работа была выполнена только для английского языка, а для русского авторы не проводили тестирование. Также, авторы не исследовали, к какому эффекту приводит использование только соседей текущего слова в дереве синтаксических зависимостей.

Целью работы является проверка гипотез: (а) добавление синтаксических свойств слова при обучении двухслойной нейронной сети (word2vec) улучшает синтаксическое сходство при подборе синонимов слова, заданного в контексте предложения на русском языке; и (б) анализ синтаксических свойств слов, расположенных в контексте заданного слова, улучшает точность подбора синонимов этого слова для русского языка.


%Также усовершенствование инструмента: переписывание вспомогательных программ для работы с моделью с языка программирования Python на язык C++, создание скрипта командной строки для запуска обучения модели, адаптация инструмента к формату входных данных, генерируемых как выходные данные синтаксическим анализатором (dependency parser) \texttt{RussianDependencyParser}\footnote{\url{https://github.com/mathtexts/RussianDependencyParser/}} для русского языка.

%Гипотеза1: добавление синтаксических свойств слова при обучении двухслойной нейронной сети (word2vec) улучшает синтаксическое сходство при подборе синонимов слова, заданного в контексте предложения на русском языке. 
%Гипотеза2: анализ синтаксических свойств слов, расположенных в контексте заданного слова, улучшает точность подбора синонимов этого слова для русского языка.




\section{\texttt{Подход на основе анализа синтаксического контекста word2vecf}}
\label{sec:Chapter3} \index{Chapter3}

На вход нашей программе подаётся текст, проанализированный зависимостным парсером ASynt'2014 от Горелова и Масленникова~\shortcite{asynt14}. Чтобы добавить информацию о синтаксических зависимостях слова, мы составляем пары слов $Word_1, Word_2$, в которой второе слово зависит от первого с типом $DependRel$. При этом получаются два токена:

\begin{center}
\(Token_1 = \) (\texttt{Word_1}, \texttt{DepRel(Word_2)});
d
\(Token_2 = \) (\texttt{Word_2}, \texttt{DepRelInv(Word_1}).
\end{center}

Например, для предложения ``У Петра I поначалу отсутствовала чёткая программа реформ'' анализируется каждое слово. Таким образом, появляются пары \texttt{<отсутствовала, circ(поначалу)>} и \texttt{<поначалу, circInv(отсутствовала)>}.
Затем на основе входного корпуса зависимостей составляется вектор пар вида $TokensVector = <(token_1, token_2)>$. Полученное множество является входными данными для инструмента \texttt{word2vecf} от \cite{levy1} .

На следующем шаге \texttt{word2vecf} использует два словаря: (а) словарь слов $TokensSet$, состоящий из множества различных элементов пар в TokensVector; и (б) словарь контекстов $ContextSet$, состоящий из множества всех различных вторых элементов ${Token_2}$. Например, в $ContextSet$ могут появиться вторые элементы \texttt{circ(поначалу)} и \texttt{circInv(отсутствовала)}.

В процессе обучения нейронная сеть предсказывает для текущей пары $<(token_1, token_2)>$  первый её элемент на основе второго. При этом из \texttt{word2vec} был заимствован подход \textit{negative sampling} для ускорения обучения:
\[\log P(\omega_i|c_i) = \log \sigma (v(\omega_i) v(c_i)) + \sum_{l=1}^s \log \sigma (- v(\omega_{neg_l}) v(c_i)).\], где $\omega_i$ и $c_i$ обозначают слово и его контекст, соответственно.
В результате, каждому из элементов в обоих словарях сопоставляется вещественный вектор размерности \(d\). Векторные представления слов и контекстов составляют веса первого и второго слоя нейронной сети, соответственно.




\section{Описание программы}
\label{sec:Chapter4} \index{Chapter4}

Входные деревья зависимостей и обучающий корпус пар вида \(p_1\) и \(p_2\) для всех имеющихся зависимостей мы сохраняем в файлах в формате конференции CoNLL-X \cite{format}.

Исходный код используемых в данной работе инструментов находится в репозитории github\footnote{\url{https://github.com/arkhipenko-ispras/mathtexts/}}.

Обучающий корпус пар может быть построен на основе корпуса зависимостей при помощи скрипта \texttt{scripts/extract\char`_deps.py} на языке Python 2 из инструмента \texttt{word2vecf} или программы \texttt{scripts/extract\char`_deps.cpp}.

\subsection{Модификация \texttt{RussianDependencyParser}}

В \texttt{RussianDependencyParser} добавлена возможность работы с файлом, содержащим большое число предложений.

Изменения касаются файлов \texttt{generateSentence.py} и \texttt{launch.sh}.

\subsection{Реализация вспомогательных программ на языке C++}

Инструмент \texttt{word2vecf} содержит два вспомогательных скрипта на языке Python 2:
\begin{itemize}
        \item
        \texttt{scripts/vocab.py} -- построение на основе корпуса зависимостей словаря, состоящего из всех различных слов, встретившихся в корпусе не менее \(threshold\) раз;
        \item
        \texttt{scripts/extract\char`_deps.py} -- построение на основе корпуса зависимостей и словаря, генерируемого предыдущим скриптом, обучающего корпуса пар.
\end{itemize}

Данные вспомогательные программы были переписаны на языке C++ с некоторыми улучшениями:
\begin{itemize}
        \item
        возможность полноценной работы с Unicode: скрипт \texttt{scripts/extract\char`_deps.py} не производит перевод русских букв в нижний регистр, во вспомогательных программах \texttt{cxx/vocab.cpp} и \texttt{cxx/extract\char`_deps.cpp} перевод осуществляется;
        \item
        удаление пар с некоторыми нежелательными типами зависимостей: \texttt{undef}, \texttt{theme}, \texttt{ex}, \texttt{punct} и \texttt{lexmod} (и обратными к ним). Реализовано в \texttt{cxx/extract\char`_deps.cpp};
        \item
        удаление пар, где хотя бы одно из слов содержит символы, отличные от букв, цифр, знака подчёркивания и дефиса, реализовано в \texttt{cxx/extract\char`_deps.cpp}.
\end{itemize}

\subsection{Адаптация \texttt{word2vecf} к \texttt{RussianDependencyParser}}

Вспомогательные программы \texttt{cxx/vocab.cpp} и \texttt{cxx/extract\char`_deps.cpp} работают с корпусом зависимостей, генерируемым \texttt{RussianDependencyParser}.

Формат этого корпуса отличается от CoNLL-X отсутствием последних двух столбцов.

\subsection{Реализация скрипта для запуска \texttt{word2vecf}}

Как часть инструмента \texttt{word2vecf} реализован скрипт командной строки \texttt{launch.sh} для запуска обучения на основе корпуса зависимостей. Автоматически при помощи вспомогательных программ на языке C++ генерируется обучающий корпус пар. Также можно настроить параметры обучения \texttt{word2vecf}, отредактировав скрипт.



\section{Эксперименты}
\label{sec:Chapter4} \index{Chapter4}

Целью наших экпериментов была проверка следующих гипотез:
Гипотеза1: добавление синтаксических свойств слова при обучении двухслойной нейронной сети (word2vec) улучшает синтаксическое сходство при подборе синонимов слова, заданного в контексте предложения на русском языке. 
Гипотеза2: анализ синтаксических свойств слов, расположенных в контексте заданного слова, улучшает точность подбора синонимов этого слова для русского языка.

В качестве данных для обучения мы использовали первые 2 миллиона строк русскоязычной Википедии~\footnote{linguatools.org/tools/corpora/wikipedia/monolingual-corpora}. С начала мы проводили зависимостный анализ текстов с помощью ASynt14 от Горелова и Масленникова~\shortcite{2014}, а затем удаляли все символы пунктуации, кроме короткого дефиса. Для тестирования мы иcпользовали методологию, применённую в Levy and Goldberg~\shortcite{melamud2015}. Они использовали показатель точности из метрики Lexical Substitution in Semeval (LS-SE), использованную для оценки систем в рамках соревнования SemEval-2007. Мы взяли 20 слов, и для каждого слова вручную подобрали 5 предложений. Для каждого предложения эксперты подобрали 5 синонимов. 

Для оценки системы использовалась стратегия, в которой система выдаёт 10 ответов. После этого точность системы оценивалась по формуле точности 
$precision = \frac{\sum_{a_i:i \in A} \frac{\sum_{res \in a_i} freq_{res}} {|H_i|} } {|A|}$, в которой А обозначает множество синонимов, аннотированных экспертами, а $H_i$ - множество ответов системы. Таким образом, эта формула усредняет показатель точности ответов на основе всех предложений. Мы использовали 3 конфигурации: (а) Baseline, соответствующий Word2vec; (b) ASub-Word, в которой в качестве контекста было подставлено слово в родительской вершине и тип его синтаксической связи с текущим словом; и (б) ASub-SyntSent, в которой в качестве контекста были использованы слова с синтаксическими пометами из всего предложения на основе множества $TokenSet$ и (в) ASub-SyntContext, в которой в качестве контекста были использованы слова с синтаксическими пометами из соседей данного слова по зависимостному дереву на основе множества $TokenSet$.   В итоге, были получены следующие результаты:

\begin{tabular}{ | c | c | c | }
\hline Конфигурация & LS-SE & LS-SE-Mode \\
\hline Baseline & LS-SE & LS-SE-Mode \\
\hline ASub-SyntSent & LS-SE & LS-SE-Mode \\
\hline ASub-SyntContext & LS-SE & LS-SE-Mode \\
\end{tabular}

В результате проверки можно сделать вывод, что гипотеза 1 и гипотеза 2 (подтверждается / опровергается).

%\include{Chapter1} % Введение
%\include{Chapter2} % Постановка задачи
%\include{Chapter3} % Обзор существующих решений
%\include{Chapter4} % Исследование и построение решения задачи
%\include{Chapter5} % Описание практической части
%\include{Chapter6} % Заключение

\nocite{*}
\bibliographystyle{gost71u} % Для соответствия требованиям об оформлении списка литературы
\bibliography{references}

\end{document}
